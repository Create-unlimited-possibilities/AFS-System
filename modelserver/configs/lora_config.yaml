# LoRA 微调配置

# 基础配置
base_model: "Qwen/Qwen2.5-7B-Instruct"
output_path: "/userdata/{user_id}/models/lora_model"

# LoRA 配置
lora:
  rank: 8
  alpha: 16
  dropout: 0.05
  target_modules:
    - q_proj
    - v_proj
    - k_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

# 训练配置
training:
  num_epochs: 3
  batch_size: 4
  gradient_accumulation_steps: 1
  learning_rate: 2e-4
  weight_decay: 0.01
  warmup_steps: 100
  max_grad_norm: 1.0
  
  save_steps: 500
  logging_steps: 10
  eval_steps: 100
  
  seed: 42

# 数据配置
data:
  train_path: "/userdata/{user_id}/datasets/sft_data.json"
  validation_split: 0.1
  max_length: 512
  padding: "max_length"

# 优化器
optimizer:
  type: "AdamW"
  betas: [0.9, 0.999]
  eps: 1e-8

# 学习率调度器
scheduler:
  type: "cosine"
  num_warmup_steps: 100

# 硬件配置
hardware:
  device: "cuda"
  num_gpus: 1
  mixed_precision: "bf16"